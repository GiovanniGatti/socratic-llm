# Socratic LLM

Using Large Language Models (LLMs) in education presents unique challenges. Typically, LLMs are designed to provide
direct answers to questions, which can hinder students' critical thinking and self-discovery skills. To address this, we
focus on fine-tuning LLMs to facilitate Socratic interactions. Instead of giving straightforward answers, these models
guide students to explore and find the answers themselves. We achieve this through Direct Preference Optimization (DPO).
We test our approach with diverse datasets, including various educational materials and Socratic dialogues. Using
advanced models like GPT-4o for evaluation, our results show that DPO successfully fine-tunes LLMs for Socratic
dialogue, enhancing their educational value.

This repository contains the source material for the paper "Fine Tuning a Large Language Model for Socratic
Interactions" (accepted at KKD-2024).

# Model inference

## HuggingFace

It's possible to download and execute the model using HuggingFace's `transformers` library with:

```python
from transformers import AutoTokenizer, AutoModelForCausalLM
import torch

model = AutoModelForCausalLM.from_pretrained(
    "eurecom-ds/Phi-3-mini-4k-socratic",
    torch_dtype=torch.bfloat16,
    trust_remote_code=True,
    device_map="cuda",
)

tokenizer = AutoTokenizer.from_pretrained("eurecom-ds/Phi-3-mini-4k-socratic", trust_remote_code=True)
```

Check out for more details at [Phi-3-mini-4k-socratic](https://huggingface.co/eurecom-ds/Phi-3-mini-4k-socratic).

## Ollama

The model is also available at
OllamaHub: [eurecom-ds/phi-3-mini-4k-socratic](https://ollama.com/eurecom-ds/phi-3-mini-4k-socratic). We also made
available the quantized versions for memory constrained environments. Ollama allows swiftly mounting this model in a web
service, or simply for local execution. For example,

```bash
# Ollama installation
curl -fsSL https://ollama.com/install.sh | sh

# Launching ollama service
ollama serve &

# Running the quantized model locally
ollama run eurecom-ds/phi-3-mini-4k-socratic:Q4_0
```

Check out more about Ollama [here](https://github.com/ollama/ollama).

![](./resources/ollama-demo.mp4)


# Scripts

We also make available evaluation scripts.

- `self_eval.py`: Perform evaluation of the LLM and prompt engineering (e.g., `GPT-4o` or `Llama3:70b`)
- `eval_model.py`: Perform evaluation of the finetuned model or the base model and prompt engineering only
- `gen_train_dataset.py`: Generates the dataset for DPO finetuning using another LLM as a judge (i.e., `GPT-4o`)
- `train.py`: Runs DPO on the base model
- `human_vs_gpt.py`: Use Judge model to perform evaluation of the human scored examples (validation of judge LLM)
- `pipeline.py`: Executes the training pipeline end-to-end (DPO dataset generation + finetuning + evaluation)

For each script, check `--help` for more details.

# Pipeline artifacts

When running the complete pipeline, the script generates a set of training and evaluation artifacts
following the given structure:

```
├── training_root                                  # name to be specified by the user
│   ├── dpo                                        # DPO related files
│   │   ├── {dataset}                              # seed dataset {mathdial,tutorchat,debugging}
│   │       ├── train_dataset.json                 # Examples generated by the base model + prompt engineering then classified in choosen/rejected by the judge model
│   │       ├── weights                            # Finetuned model weights
│   │       ├── checkpoints                        # Training checkpoints
│   ├── evaluation                                 # Performance assements related files
│   │   ├── {dataset}                              # seed dataset {mathdial,tutorchat,debugging}
│   │   │   ├── from_finetuned_with_tutorchat.json # GPT-4o evaluation using model finetuned with tutorchat data 
│   │   │   ├── from_finetuned_with_mathdial.json  #    "        "       "     "       "      "   mathdial data
│   │   │   ├── from_finetuned_with_debugging.json #    "        "       "     "       "      "   debbuging data
│   │   │   ├── base.json                          #    "        "       "     base model + prompt-engineering
│   │   │   ├── gpt4o.json                         #    "        "       "     GPT-4o + prompt-engineering
│   │   ├── human_vs_gpt.json                      # Comparison between human asssessment and judge LLM
│   ├── figures                                    # report evaluation figures
```

# Running in Docker container

It's possible to run any project's script with a Docker container. To do so, first build the image with

```bash
$  docker build -t socratic-llm .
```

Then run it with (tip: don't forget to mount the GPU and script's input/output directories). For example,

```bash
$ docker run --rm --gpus all -v socratic-llm/:/socractic-llm -v /home/<user>/huggingface:/huggingface -e HF_HOME=/huggingface -it socratic-llm -m pipeline --judge-llm openai <open-ai-key> gpt-4o --output-dir /socractic-llm --instruct-model microsoft/Phi-3-mini-4k-instruct
```
